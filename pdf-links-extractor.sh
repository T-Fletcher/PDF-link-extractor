#!/bin/sh

# PDF link extractor for Linux. Requires lynx, pdftohtml and qpdf

# This script will search for PDFs, and extract any hyperlinks within,
# then save the results to 2 files: one with everything, another with 
# only duplicates
#
# It does this by:
#
# 1. Search the current directory for PDF files
# 2. Uncompresses each PDF it finds into a new file
# 3. Generate a HTML version of each uncompressed file
# 4. Searches the HTML file for HTTP-based links
# 5. Appends the links into output files
# 6. Sorts the contents of these files, then saves it into new datestamped files
# 7. Prefixes the new files with CSV labels to describe each captured field
# 8. Removes the temporary files the script creates
# 

# Variables

START=`date +%s`
FORMAT="csv"
DELIMITER="|"
OUTPUT="_pdflinks_dump.tmp"
RESULTS="_pdflinks-"$(date -d "today" +"%Y%m%d-%H%M%S")"."$FORMAT
DUPLICATES="_pdflinks-"$(date -d "today" +"%Y%m%d-%H%M%S")"_duplicates."$FORMAT
HR="------------------------------------------"


echo $HR
echo ">>>---- GO GO PDF LINK EXTRACTOR!!! ---->"
echo $HR
echo 
echo "Checking all the PDFs in here for links..."
echo


# Remove any tmp files generated by this script

if [ -e $OUTPUT ]; then
  rm $OUTPUT
fi

# Search all PDFs under the current working directory

find . -type f -name '*.pdf' | while read -r file; do
 
  # Uncompress each PDF file as much as possible. 'Uncompress' ...or 'decompress'.
  qpdf --stream-data=uncompress --decode-level=all "$file" "$file-2"


  # Generate a HTML copy of the new PDF, leaving out bulky content like images
  pdftohtml "$file-2" -s -i -q -noframes


  # Search the new file for links, and append the output into a results file
  lynx -dump -nonumbers -listonly "$file-2.html" | grep 'http\|mailto:' | while read -r link
  
  do
    echo $file$DELIMITER$link$DELIMITER >> $OUTPUT
  done  


  # Remove the generated uncompressed file once it's served its purpose, as 
  # these can be huge and could eat your hard drive :$

  rm -f "$file-2" "$file-2.html"

done


# Note how long the script took to run

END=`date +%s`
TIMER_SECONDS=$((END-START))

echo
printf 'Check completed in %02dh:%02dm:%02ds\n' $(($TIMER_SECONDS/3600)) $(($TIMER_SECONDS%3600/60)) $(($TIMER_SECONDS%60))
echo

# Check if there are any results - if not, the $OUTPUT file will be empty

if [ ! -s $OUTPUT ]; then

  echo
  echo "No links found!"
  echo

else	

  # Sort the results alphabetically, save to a new file
  sort $OUTPUT > $RESULTS

  # Count the results
  COUNT=$(wc -l < $RESULTS)

  # Export any duplicate results (case insensitive) and sort them alphabetically. 
  # Counting and more fiddly sorting can then be done using a CSV program like Excel.
  uniq -Ddi $RESULTS | sort -hr > $DUPLICATES
 
  # Count the number of links that have at least one duplicate result somwhere
  DUPLICATES_COUNT=$(wc -l < $DUPLICATES)
  
  # Remove the original output
  rm $OUTPUT

  echo
  echo $COUNT" results found. "$DUPLICATES_COUNT" link/s were found to have at least 1 duplicate, though not necessarily within the same PDF document."
  echo
  echo "See "$RESULTS" for your links"
  echo "See "$DUPLICATES" for the list of duplicate links"
  echo $HR 
  echo
fi


# lastly, populate the output files with CSV headings

# echo "duplicates_found"$DELIMITER"file_of_origin"$DELIMITER"link"$DELIMITER | cat - $DUPLICATES > tmp && mv tmp $DUPLICATES
echo "file_of_origin"$DELIMITER"link"$DELIMITER | cat - $DUPLICATES > tmp && mv tmp $DUPLICATES
echo "file_of_origin"$DELIMITER"link"$DELIMITER | cat - $RESULTS > tmp && mv tmp $RESULTS


# Doneski

# @TODO: 
#
# Allow the script to receive arguments: delimiter character, counter y/n, include filepath y/n, include email links y/n
# Cull file the path, only export the filename and extension? Make this optional
# The duplicates file doesn't include a delimiter between the counter number and file of origin when counting is used
# The duplicates counter includes a count, then a filepath, but it isn't clear what the filepath represents
# Include an email link vs navigation link counter
# Export email links to a different file, as they maysensitive information
# Make tracking/logging unique links optional
